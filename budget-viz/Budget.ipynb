{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06080d3d",
   "metadata": {},
   "source": [
    "\n",
    "# TODO:\n",
    "- Pull data from other sources (retirement/dividend etc...)\n",
    "- Fix the fucking pulled from saving (both ways are triggered, it is fine except monthly sum migh be negative enough to show up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8560c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of requirements, it really depend on your environment\n",
    "## Kinda very needed\n",
    "# !pip instal python_dateutil # or install based on you your env \n",
    "# !conda install -c conda-forge numpy # installed with conda\n",
    "# !conda install -c conda-forge pandas #  installed with conda \n",
    "\n",
    "## Make pretty graph notebook\n",
    "# !conda install -c conda-forge matplotlib  \n",
    "# !conda install -c plotly plotly \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e68eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import plotly.graph_objects as go \n",
    "import plotly.express as px \n",
    "import pprint\n",
    "\n",
    "## This contains the actual classification mapping\n",
    "import dictdata \n",
    "\n",
    "### These might need to be tuned to match display\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "mpl.rcParams['font.size'] = 200.0\n",
    "mpl.rcParams['font.size'] = 200.0\n",
    "\n",
    "# Global variables And settings\n",
    "YEAR_IN_REVIEW=2021\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "pd.set_option('display.max_rows', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e2fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439291ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function(s)\n",
    "def clean_currency(x):\n",
    "    \"\"\" If the value is a string, then remove wrong delimeter. Ideally this would be leveraging LC_NUMERIC\n",
    "    Or if very lazy or scramble data something like https://github.com/hayj/SystemTools/blob/master/systemtools/number.py\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        return(x.replace(',', '.'))\n",
    "    return(x)\n",
    "\n",
    "# Not used, by this project at least\n",
    "# def parseAmount(amountString):\n",
    "#     try:\n",
    "#         return float(str(amountString.replace(\",\", \".\")))\n",
    "#     except ValueError as ve:\n",
    "#         print(ve)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aca5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XXX Ideally a way to upload the csv directly via a widget and/or UI\n",
    "path=\"/home/lfx/Download/2021.tab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a93426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is where you match whatever format you have in the file and give it the following header\n",
    "df = pd.read_csv(path,delimiter='\\t',header=None,names=['Account Number','Currency','Date','Amount before transaction','Amount after transaction','Date of transaction','Transaction','Note'])\n",
    "df['Transaction'] = df['Transaction'].apply(clean_currency).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6a342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show a \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a2029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# More global data\n",
    "full_value = 0\n",
    "\n",
    "# we want a dict that will hold our results of aggregate values by catergory\n",
    "value_by_category = {}\n",
    "\n",
    "# for coolness reason we also track the spend on each shop\n",
    "value_by_shop = {}\n",
    "\n",
    "# we want a dict that will hold our spent per card\n",
    "value_by_card = {}\n",
    "\n",
    "# and here we will track all the lines that we could not match to a shop\n",
    "unknown_shops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805c1de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This is probably wrong and ugly, iterrows iterate over a series but not the original df ?\n",
    "## Also the repeated usage of \"row[X]\" is unsettling \n",
    "for index, row in df.iterrows():\n",
    "    amount = row['Transaction']\n",
    "    # if we caught an exception parsing, then we won't have an amount here\n",
    "    # instead it will be `None`, so ignore it and move on to the next item in the loop\n",
    "    if amount is None:\n",
    "        continue\n",
    "    if amount <0:\n",
    "        df.at[index,'Out']=abs(amount)\n",
    "        df.at[index,'In']=0.0\n",
    "    else:\n",
    "        df.at[index,'Out']=0.0\n",
    "        df.at[index,'In']=amount\n",
    "\n",
    "    # ok this is shit and nasty it took me some time to realize that you don't have the shop name cleanly\n",
    "    # and it's just a comment field with everything.\n",
    "    # Ok now we want to optimize the loop. What you are doing in your if/elif block is:\n",
    "    #   You take the item from row[7]\n",
    "    #   Loop over each list in a hardcoded way\n",
    "    #   If you match you add it to a hardcoded aggregate value per category.\n",
    "\n",
    "    # First we take the string from row[7]\n",
    "    shop_string = row['Note']\n",
    "    found = False\n",
    "    ## Add field \"Date Of Execution\" date string with actual date because replacing is hard and i am lazy\n",
    "    date_time_obj = datetime.strptime(str(row['Date of transaction']), '%Y%m%d')\n",
    "    df.at[index,'Date Of Execution']=date_time_obj\n",
    "    for category, shops in list(dictdata.shops_in_category.items()):\n",
    "        #print(dictdata.shops_in_category.items())\n",
    "        for shop in shops:\n",
    "            if shop in shop_string:\n",
    "                found = True\n",
    "                #print(\"shop: \" + shop +\" category  \"+ category)\n",
    "                df.at[index,'tags']=category\n",
    "                # Now if the shop is in this category we want to add the amount spent to the aggregate of amounts spent for this category:\n",
    "                if category in value_by_category:\n",
    "                    value_by_category[category] += amount\n",
    "                else:\n",
    "                    value_by_category[category] = amount\n",
    "\n",
    "                # We also add it to the total spend per shop:\n",
    "                if shop in value_by_shop:\n",
    "                    value_by_shop[shop] += amount\n",
    "                else:\n",
    "                    value_by_shop[shop] = amount\n",
    "                    # If we don't want a shop to belong to multiple categories, then as soon as we match it [this `if` statement executes] we want to stop looping, we can do that by using a `break` statement:\n",
    "                    # If you want to have the same shop in multiple categories, then just remove this `break` statement.\n",
    "                break\n",
    "        # Let's keep track of all the ones we didn't find so we can categorize them\n",
    "    if not found:\n",
    "        unknown_shops.append(shop_string)\n",
    "        df.at[index,'tags']= \"UNKNOWN\"\n",
    "        #print(shop_string)\n",
    "\n",
    "        # Now we also want to add the amount to the total value:\n",
    "    full_value += amount\n",
    "    \n",
    "\n",
    "    paid_by_card = False\n",
    "    ## While we are looping lets get spend per card holder, this rely on PASXXX in the TSV file\n",
    "    for owner,cards in list(dictdata.card_owner.items()):\n",
    "        for card in cards:\n",
    "            if card in shop_string:\n",
    "                paid_by_card = True\n",
    "                df.at[index,'Card Holder']=owner\n",
    "                # Now if the shop is in this category we want to add the amount spent to the aggregate of amounts spent for this category:\n",
    "                if card in value_by_card:\n",
    "                    value_by_card[card] += amount\n",
    "                else:\n",
    "                    value_by_card[card] = amount\n",
    "\n",
    "    \n",
    "    if not paid_by_card:\n",
    "        df.at[index,'Card Holder']=\"automatic\"\n",
    "        if \"automatic\" in value_by_card:\n",
    "            value_by_card[\"automatic\"] += amount\n",
    "        else:\n",
    "            value_by_card[\"automatic\"] = amount\n",
    "\n",
    "## ITs this to iterate a second time but i would rather not fuck it up, again            \n",
    "for index, row in df.iterrows():\n",
    "    tags = row['tags']\n",
    "    budget_type_found = False\n",
    "    for budget_type,tag_value  in list(dictdata.category_in_Budget.items()):\n",
    "        for tag in tag_value:\n",
    "            if tags in tag:\n",
    "                budget_type_found = True\n",
    "                df.at[index,'Budget Type']=budget_type\n",
    "        if not budget_type_found:\n",
    "            df.at[index,'Budget Type']=\"Wants\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df3b58",
   "metadata": {},
   "source": [
    "### This is the part of the feedback loop if your dictdata.py is empty\n",
    "Running this cell will return a list of \"unknown\" shops. Usually you would have a look and pick the largest/unique string that qualify the most shop of the same type and update \"dictdata.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9875de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pp.pprint('full_value %s' % (full_value))\n",
    "#pp.pprint(value_by_category)\n",
    "#pp.pprint(value_by_shop)\n",
    "pp.pprint('%d unknown shops' % len(unknown_shops))\n",
    "for unknown in unknown_shops:\n",
    "    print(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Not displayed but shops in subgroups are also collected \n",
    "# This is ugly and old in the current iter, we have In/Out fields and shouldnt rely on abs()\n",
    "value_by_shop_list = []\n",
    "for k in value_by_shop.items():\n",
    "    val=(k[0],abs(k[1]))\n",
    "    value_by_shop_list.append(val)\n",
    "\n",
    "df_value_by_shop = pd.DataFrame.from_records(\n",
    "    value_by_shop_list, columns=['Shop', 'Total Spent'])\n",
    "\n",
    "df_value_by_shop.loc[df_value_by_shop['Total Spent'] < 500, 'Shop'] = 'Other vendor' # Represent only large shops\n",
    "fig = px.pie(df_value_by_shop, values='Total Spent', names='Shop', title='Euro Value per vendor')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a678884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save all income in a single CSV file\n",
    "Income = df.loc[df['tags'].isin(['Salary','Dividends','Refunds'])]\n",
    "Income.to_csv(r'Incomedata.csv')\n",
    "# Save all the data back with all the categories \n",
    "df.to_csv(r'Fulldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ccb43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grouping: https://towardsdatascience.com/how-to-group-data-by-different-time-intervals-using-python-pandas-eb7134f9b9b0\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n",
    "\n",
    "#grouped = df.groupby([pd.Grouper(key='Date Of Execution', freq='M'), 'tags']).Transaction.sum().plot()\n",
    "\n",
    "## Result is a series, using to frame to just not bother \n",
    "grouped = df.groupby([pd.Grouper(key='Date Of Execution', freq='M'), 'tags','Budget Type','Card Holder']).Transaction.sum().to_frame()\n",
    "\n",
    "for index,row in grouped.iterrows():\n",
    "    amount = row['Transaction']\n",
    "    if amount <0:\n",
    "        grouped.at[index,'Out']=abs(amount)\n",
    "        grouped.at[index,'In']=0.0\n",
    "    if amount > 0:\n",
    "        grouped.at[index,'Out']=0.0\n",
    "        grouped.at[index,'In']=amount\n",
    "\n",
    "#print(grouped.sort_values(by=['Date Of Execution'])) # No longer needed as it is sorted by default \n",
    "# Saved grouped to CSV\n",
    "grouped.to_csv(r'grouped.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391943dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### XXX This is ugly as sin and probably the worse way to do this\n",
    "### XXX Column name is wrong, it should be period and it should be just \"YYYY-MM\"\n",
    "### Instead i am being lazy and picking \"YYYY-MM-last-day\"\n",
    "\n",
    "### In the mean time we iterate over the dates but we pull data from the file we saved earlier\n",
    "# XXX df_grouped is used heavily because due to lazyness the group_by  in the above cell \n",
    "#     create a different index (tag is no long a column by itself)\n",
    "dataframes_per_month = {}\n",
    "df_grouped = pd.read_csv('grouped.csv')\n",
    "\n",
    "for month in range(1,13):\n",
    "     ## Worse way to get the last day of the year\n",
    "     monthly=date(YEAR_IN_REVIEW,month,1)\n",
    "     monthly=monthly + relativedelta(day=31)\n",
    "     \n",
    "     #print(monthly)\n",
    "     df3_monthly_temp = df_grouped.loc[df_grouped['Date Of Execution'] == monthly.strftime('%Y-%m-%d')].sort_values(by='Transaction')\n",
    "     dataframes_per_month[monthly]=df3_monthly_temp\n",
    "\n",
    "     titleofgraph = (\"Break down for \"+str(monthly)[:-3])\n",
    "\n",
    "    \n",
    "     fig=px.sunburst(dataframes_per_month[monthly],values='Out',path=[\"Budget Type\",\"tags\",\"Card Holder\"],\n",
    "                     width=800,title = titleofgraph)\n",
    "     fig.update_traces(textinfo='label+percent root+value')\n",
    "     fig.show()\n",
    "     #fig.write_image(\"output/\"+str(month).zfill(2)+\".png\", engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421924df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point i am just trying around different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8509c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yearly_category_sums = abs(grouped.groupby(\"tags\").sum())\n",
    "ax = Yearly_category_sums.plot.pie(y=\"Transaction\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c4c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yearly_category_sums.sort_values(by=['Transaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8676318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX DEFO does not work with lots of smaller percentage, the display is bad.\n",
    "df_yearly_breakdown = df_grouped.copy(deep=True)\n",
    "df_yearly_breakdown.loc[df_yearly_breakdown['Out'] < 200, 'tags'] = 'Other shops' # Represent only large purchases/transactions\n",
    "fig = px.pie(\n",
    "     df_grouped, values = 'Out',\n",
    "        names = 'tags', title = 'Break down of the full year',\n",
    "        color_discrete_sequence = px.colors.sequential.Magma )\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yearly_breakdown.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec97d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78a6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
